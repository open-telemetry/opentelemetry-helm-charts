# Top level field indicating an override for fullname
fullnameOverride: ""
# Top level field indicating an override for the namespace
namespaceOverride: ""

# Top level field specifying the name of the cluster
clusterName: ""

# Extra environment variables to add to each collector, bridge and instrumentation
extraEnvs: []

# Should the CRDs be installed by this chart.
crds:
  # Should the CRDs be installed
  install: true

# Top level field related to the OpenTelemetry Operator
opentelemetry-operator:
  # Field indicating whether the operator is enabled or not
  # This is disabled for now while the chart is under development
  enabled: false

  # Sub-field for admission webhooks configuration
  admissionWebhooks:
    # Policy for handling failures
    # Setting this allows for an installation of the otel operator at the same time as the custom resources it manages.
    failurePolicy: "Ignore"

  # This is disabled by default, as doing so creates a race condition with helm.
  # https://github.com/open-telemetry/opentelemetry-helm-charts/issues/677
  # Users of this chart should _never_ set this to be true. If a user wishes
  # to install the CRDs through the opentelemetry-operator chart, it is recommended
  # to install the opentelemetry-operator chart separately and prior to the installation
  # of this chart.
  crds:
    create: false

# This is the default configuration for all collectors generated by the chart.
# Any collectors in the `collectors` are overlayed on top of this configuration.
defaultCRConfig:
  enabled: false

  # Name of the collector
  name: "collector"

  # Annotations for the collector
  annotations: {}
  #   io.opentelemetry.com/resource: hello

  # Labels for the collector
  labels: {}
  #   app: otc

  # Management state of the collector
  managementState: managed

  # Configuration for cluster role binding
  clusterRoleBinding:
    enabled: true
    clusterRoleName: ""

  # Number of replicas for the collector
  # replicas: 1

  # Mode of deployment for the collector
  mode: deployment

  # Service account associated with the collector
  serviceAccount: ""

  # Image details for the collector
  image:
    # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
    repository: otel/opentelemetry-collector-contrib
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
    digest: ""

  # Upgrade strategy for the collector
  upgradeStrategy: automatic

  # Configuration options for the collector
  config: {}
  # receivers:
  #   otlp:
  #     protocols:
  #       grpc:
  #         endpoint: ${env:MY_POD_IP}:4317
  #       http:
  #         endpoint: ${env:MY_POD_IP}:4318
  # exporters:
  #   otlp:
  #     endpoint: "otel-collector.default:4317"
  #     tls:
  #       insecure: true
  #     sending_queue:
  #       num_consumers: 4
  #       queue_size: 100
  #     retry_on_failure:
  #       enabled: true
  # processors:
  #   batch:
  #   memory_limiter:
  #     # 80% of maximum memory up to 2G
  #     limit_mib: 400
  #     # 25% of limit up to 2G
  #     spike_limit_mib: 100
  #     check_interval: 5s
  # extensions:
  #   zpages: {}
  # service:
  #   extensions: [zpages]
  #   pipelines:
  #     traces:
  #       receivers: [otlp]
  #       processors: [memory_limiter, batch]
  #       exporters: [otlp]

  # Whether to use host network for the collector
  hostNetwork: false

  # Whether to share process namespace for the collector
  shareProcessNamespace: false

  # Priority class name for the collector
  priorityClassName: ""

  # Termination grace period for the collector
  terminationGracePeriodSeconds: 30

  # Resource requests and limits for the collector
  resources:
    requests:
      memory: "64Mi"
      cpu: "250m"
    limits:
      memory: "128Mi"
      cpu: "250m"

  # Node selector for the collector
  nodeSelector: {}
  #   nodeType: worker

  # Arguments for the collector
  args: {}
  #   arg1: value1
  #   arg2: value2

  # Autoscaler configuration for the collector
  autoscaler: {}
  #   minReplicas: 1
  #   maxReplicas: 10
  #   targetCPUUtilization: 50

  # Pod disruption budget for the collector
  podDisruptionBudget: {}
  #   maxUnavailable: 1

  # Security context for the collector
  securityContext: {}
  #   runAsUser: 1000
  #   capabilities:
  #     drop:
  #       - ALL

  # Pod security context for the collector
  podSecurityContext: {}
  #   runAsUser: 1000

  # Annotations for the collector's pods
  podAnnotations: {}
  #   prometheus.io/scrape: "true"

  # Target allocator configuration
  targetAllocator: {}
  # replicas: 1
  # nodeSelector:
  #   nodeType: worker
  # resources:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"
  # allocationStrategy: consistent-hashing
  # filterStrategy: relabel-config
  # serviceAccount: my-service-account
  # image: myregistry/myimage:latest
  # enabled: true
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #         - matchExpressions:
  #             - key: kubernetes.io/e2e-az-name
  #               operator: In
  #               values:
  #                 - e2e-az1
  #                 - e2e-az2
  # # Configuration for Prometheus Custom Resources
  # prometheusCR:
  #   enabled: true
  #   scrapeInterval: 30s
  #   podMonitorSelector:
  #     key1: value1
  #     key2: value2
  #   serviceMonitorSelector:
  #     key1: value1
  #     key2: value2
  # securityContext:
  #   runAsUser: 1000
  #   capabilities:
  #     drop:
  #       - ALL
  # podSecurityContext:
  #   runAsUser: 1000
  # # Topology spread constraints for the target allocator
  # topologySpreadConstraints:
  #   - maxSkew: 1
  #     topologyKey: kubernetes.io/hostname
  #     whenUnsatisfiable: DoNotSchedule
  # # Tolerations for the collector
  # tolerations:
  #   - key: "key"
  #     operator: "Equal"
  #     value: "value"
  #     effect: "NoSchedule"
  # # Environment variables for the target allocator
  # env:
  #   - name: ENV_VAR1
  #     value: value1
  #   - name: ENV_VAR2
  #     value: value2
  # # Observability configuration for the target allocator
  # observability:
  #   metrics:
  #     enableMetrics: true

  # Affinity configuration for the collector
  affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #         - matchExpressions:
  #             - key: kubernetes.io/e2e-az-name
  #               operator: In
  #               values:
  #                 - e2e-az1
  #                 - e2e-az2

  # Lifecycle configuration for the collector
  lifecycle: {}
  #   preStop:
  #     exec:
  #       command:
  #         [
  #           "/bin/sh",
  #           "-c",
  #           "echo Hello from the preStop handler > /dev/termination-log",
  #         ]

  # Liveness probe configuration for the collector
  livenessProbe: {}
  #   initialDelaySeconds: 3
  #   periodSeconds: 5
  #   timeoutSeconds: 2
  #   failureThreshold: 5

  # Observability configuration for the collector
  observability: {}
  #   metrics:
  #     enableMetrics: true

  # Update strategy for the collector
  updateStrategy: {}
  #   type: RollingUpdate

  # Volume mounts for the collector
  volumeMounts: []
  #   - name: data
  #     mountPath: /data

  # Ports configuration for the collector
  # The operator automatically calculates ports for known receivers and exporters
  # Set any custom ports here.
  ports: []
  # - name: http
  #   protocol: TCP
  #   port: 80
  #   targetPort: 8080

  # Environment variables for the collector
  env: []
  # - name: ENV_VAR1
  #   value: value1
  # - name: ENV_VAR2
  #   value: value2

  # Volume claim templates for the collector
  volumeClaimTemplates: []
  # - metadata:
  #     name: storage
  #   spec:
  #     accessModes: ["ReadWriteOnce"]
  #     resources:
  #       requests:
  #         storage: 1Gi

  # Tolerations for the collector
  tolerations: []
  # - key: "key"
  #   operator: "Equal"
  #   value: "value"
  #   effect: "NoSchedule"

  # Volumes for the collector
  volumes: []
  # - name: config-volume
  #   configMap:
  #     name: config

  # Init containers for the collector
  initContainers: []
  # - name: init-nginx
  #   image: nginx

  # Additional containers for the collector
  additionalContainers: []
  # - name: additional-container
  #   image: busybox

  # Topology spread constraints for the collector
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: kubernetes.io/hostname
  #   whenUnsatisfiable: DoNotSchedule
  #   labelSelector:
  #     matchLabels:
  #       app: my-app

  # Config maps for the collector
  configmaps: []
  # - name: config
  #   mountPath: /etc/config

# Collectors is a map of collector configurations of the form:
# collectors:
#   collectorName:
#     enabled: true
#     name: "example"
# Each collector configuration is layered on top of the `defaultCRConfig`, overriding a default if set.
# This configuration allows for multiple layers of overrides for different clusters. For example, you could
# create a collector called test with an OTLP exporter in your values.yaml, and then override the endpoint's
# destination in a file called values-staging.yaml.
collectors: {}

# Cluster role configuration
clusterRole:
  # Whether the cluster role is enabled or not
  enabled: false

  # Annotations for the cluster role
  annotations: {}

  # Rules for the cluster role
  rules: []

# Instrumentation configuration
instrumentation:
  # Whether instrumentation is enabled or not
  enabled: false
  labels: {}
  annotations: {}

  # Exporter configuration
  exporter:
    # This is the default collector's service
    # Upon creation of a tracing collector, edit this endpoint.
    endpoint: http://collector-collector:4317

  # Resource configuration
  resource:
    resourceAttributes: {}
    # environment: dev
    addK8sUIDAttributes: true

  # Propagators configuration
  propagators:
    - tracecontext
    - baggage
    - b3
    - b3multi
    - jaeger
    - xray
    - ottrace

  # Sampler configuration
  sampler: {}
  # type: parentbased_always_on
  # argument: "0.25"

  # Environment variables for instrumentation
  env: []
  # - name: ENV_VAR1
  #   value: value1
  # - name: ENV_VAR2
  #   value: value2

  # Java agent configuration
  java: {}
  # image: myregistry/java-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: JAVA_ENV_VAR
  #     value: java_value
  # resources:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # NodeJS agent configuration
  nodejs: {}
  # image: myregistry/nodejs-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: NODEJS_ENV_VAR
  #     value: nodejs_value
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # Python agent configuration
  python: {}
  # image: myregistry/python-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  # - name: PYTHON_ENV_VAR
  #   value: python_value
  # #  Required if endpoint is set to 4317.
  # #  Python autoinstrumentation uses http/proto by default
  # #  so data must be sent to 4318 instead of 4317.
  # - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #   value: http://otel-collector:4318
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # .NET agent configuration
  dotnet: {}
  # image: myregistry/dotnet-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: DOTNET_ENV_VAR
  #     value: dotnet_value
  #   # Required if endpoint is set to 4317.
  #   # Dotnet autoinstrumentation uses http/proto by default
  #   # See https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/blob/888e2cd216c77d12e56b54ee91dafbc4e7452a52/docs/config.md#otlp
  #   - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #     value: http://otel-collector:4318
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # Go agent configuration
  go: {}
  # image: myregistry/go-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: GO_ENV_VAR
  #     value: go_value
  #   # Required if endpoint is set to 4317.
  #   # Dotnet autoinstrumentation uses http/proto by default
  #   # See https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/blob/888e2cd216c77d12e56b54ee91dafbc4e7452a52/docs/config.md#otlp
  #   - name: OTEL_EXPORTER_OTLP_ENDPOINT
  #     value: http://otel-collector:4318
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # Apache HTTPd agent configuration
  apacheHttpd: {}
  # image: myregistry/apache-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: APACHE_ENV_VAR
  #     value: apache_value
  # attrs:
  #   - name: ATTRIBUTE_VAR
  #     value: attribute_value
  # version: "2.4"
  # configPath: "/usr/local/apache2/conf"
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

  # NGINX agent configuration
  nginx: {}
  # image: myregistry/nginx-agent:latest
  # volumeLimitSize: 200Mi
  # env:
  #   - name: NGINX_ENV_VAR
  #     value: nginx_value
  # attrs:
  #   - name: ATTRIBUTE_VAR
  #     value: attribute_value
  # configFile: "/etc/nginx/nginx.conf"
  # resourceRequirements:
  #   requests:
  #     memory: "64Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "128Mi"
  #     cpu: "500m"

# OpAMP bridge configuration. The OpAMP Bridge is an OpenTelemetry component
# that enables enhanced configuration and health monitoring for OpenTelemetry collectors
# deployed in Kubernetes. The Bridge pulls collector CRDs from the Kubernetes cluster and
# reports their configuration and status to a remote OpAMP Server. The Bridge will only pull
# collectors labeled with either
# * opentelemetry.io/opamp-reporting: true
# * opentelemetry.io/opamp-managed: true
# You can learn more about the Bridge's design here:
# https://docs.google.com/document/d/1M8VLNe_sv1MIfu5bUR5OV_vrMBnAI7IJN-7-IAr37JY
opAMPBridge:
  # Whether OpAMP bridge is enabled or not
  enabled: false

  # Adds `opentelemetry.io/opamp-reporting: true` to all collectors
  addReportingLabel: true
  # Adds `opentelemetry.io/opamp-managed: true` to all collectors
  addManagedLabel: false

  # Endpoint for OpAMP server
  endpoint: http://opamp-server:8080

  # Headers configuration for OpAMP bridge
  headers: {}
  # Authorization: Bearer your_access_token
  # Custom-Header: Custom-Value

  # Capabilities of OpAMP bridge
  # You can learn more about OpAMP's capabilities here:
  # https://github.com/open-telemetry/opamp-spec/blob/main/specification.md#agenttoservercapabilities
  capabilities:
    AcceptsOpAMPConnectionSettings: true
    AcceptsOtherConnectionSettings: true
    AcceptsRemoteConfig: true
    AcceptsRestartCommand: true
    ReportsEffectiveConfig: true
    ReportsHealth: true
    ReportsOwnLogs: true
    ReportsOwnMetrics: true
    ReportsOwnTraces: true
    ReportsRemoteConfig: true
    ReportsStatus: true

  # Components allowed for OpAMP bridge
  componentsAllowed: {}
  # receiver:
  #   - otlp
  #   - prometheus
  # processor:
  #   - batch
  #   - memory_limiter
  # exporter:
  #   - prometheusremotewrite

  # Resources configuration for OpAMP bridge
  resources:
    limits:
      cpu: "250m"
      memory: "256Mi"
    requests:
      cpu: "250m"
      memory: "256Mi"

  # Security context for OpAMP bridge
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000

  # Pod security context for OpAMP bridge
  podSecurityContext:
    fsGroup: 1000

  # Pod annotations for OpAMP bridge
  podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "8080"

  # Service account for OpAMP bridge
  serviceAccount: ""

  # Image for OpAMP bridge
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-operator/operator-opamp-bridge
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
    digest: ""

  # Upgrade strategy for OpAMP bridge
  upgradeStrategy: automatic

  # Volume mounts for OpAMP bridge
  volumeMounts: []
  # - name: data
  #   mountPath: /data

  # Ports configuration for OpAMP bridge
  ports: []
  # - name: http
  #   port: 8080
  #   protocol: TCP

  # Environment variables for OpAMP bridge
  env: []
  # - name: ENVIRONMENT
  #   value: production

  # Environment variables from config map for OpAMP bridge
  envFrom: []
  # - configMapRef:
  #     name: opamp-config

  # Tolerations for OpAMP bridge
  tolerations: []
  # - key: "opamp"
  #   operator: "Equal"
  #   value: "true"
  #   effect: "NoSchedule"

  # Volumes for OpAMP bridge
  volumes: []
  # - name: data
  #   emptyDir: {}

  # Whether to use host network for OpAMP bridge
  hostNetwork: false

  # Priority class name for OpAMP bridge
  priorityClassName: ""

  # Affinity configuration for OpAMP bridge
  affinity: {}
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #       - matchExpressions:
  #           - key: opamp
  #             operator: In
  #             values:
  #               - "true"

  # Topology spread constraints for OpAMP bridge
  topologySpreadConstraints: []
  # - maxSkew: 1
  #   topologyKey: "kubernetes.io/hostname"
  #   whenUnsatisfiable: "DoNotSchedule"
  #   labelSelector:
  #     matchLabels:
  #       opamp: "true"

  # Bridge cluster role configuration
  # In order to function the bridge is given its default role to
  # list and get pods and opentelemetry collectors
  clusterRole:
    # Whether the bridge cluster role is enabled or not
    enabled: true

    # Annotations for the bridge cluster role
    annotations: {}

    # Rules for the bridge cluster role
    rules: []
