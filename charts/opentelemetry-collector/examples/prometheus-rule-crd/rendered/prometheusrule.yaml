---
# Source: opentelemetry-collector/templates/prometheusrule.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: example-opentelemetry-collector
  namespace: default
  labels:
    helm.sh/chart: opentelemetry-collector-0.142.1
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: example
    app.kubernetes.io/version: "0.142.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: opentelemetry-collector
    app.kubernetes.io/component: agent-collector
spec:
  groups:
  - name: collectorRules
    rules:
    - alert: ReceiverDroppedSpans
      expr: rate(otelcol_receiver_refused_spans_total[5m]) > 0
      for: 2m
      labels:
        severity: critical
      annotations:
        description: 'The {{ $labels.receiver }} receiver is dropping spans at a rate of {{ humanize $value }} per second '
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#receive-failures'
    - alert: ReceiverDroppedMetrics
      expr: rate(otelcol_receiver_refused_metric_points_total[5m]) > 0
      for: 2m
      labels:
        severity: critical
      annotations:
        description: 'The {{ $labels.receiver }} receiver is dropping metrics at a rate of {{ humanize $value }} per second '
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#receive-failures'
    - alert: ReceiverDroppedLogs
      expr: rate(otelcol_receiver_refused_log_records_total[5m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        description: ' The {{ $labels.receiver }} is dropping logs at a rate of {{ humanize $value }} per second '
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#receive-failures'
    - alert: ExporterDroppedSpans
      expr: rate(otelcol_exporter_send_failed_spans_total[5m]) > 0
      for: 2m
      labels:
        severity: critical
      annotations:
        description: 'The {{ $labels.exporter }} exporter is dropping spans at a rate of {{ humanize $value }} per second '
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#receive-failures'
    - alert: ExporterDroppedMetrics
      expr: rate(otelcol_exporter_send_failed_metric_points_total[5m]) > 0
      for: 2m
      labels:
        severity: critical
      annotations:
        description: 'The {{ $labels.exporter }} exporter is dropping metrics at a rate of {{ humanize $value }} per second '
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#receive-failures'
    - alert: ExporterDroppedLogs
      expr: rate(otelcol_exporter_send_failed_log_records_total[5m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        description: ' The {{ $labels.exporter }} is dropping logs at a rate of {{ humanize $value }} per second '
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#receive-failures'
    - alert: ExporterQueueSize
      expr: otelcol_exporter_queue_size > otelcol_exporter_queue_capacity * 0.8
      for: 1m
      labels:
        severity: warning
      annotations:
        description: 'The {{ $labels.exporter }} queue has reached a size of {{ $value }} '
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#queue-length'
    - alert: SendQueueFailedspans
      expr: rate(otelcol_exporter_enqueue_failed_spans_total[5m]) > 0
      for: 1m
      labels:
        severity: warning
      annotations:
        description: 'The {{ $labels.exporter }} sending queue failed to accept {{ $value }}  spans'
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#queue-length'
    - alert: SendQueueFailedmetric_points
      expr: rate(otelcol_exporter_enqueue_failed_metric_points_total[5m]) > 0
      for: 1m
      labels:
        severity: warning
      annotations:
        description: 'The {{ $labels.exporter }} sending queue failed to accept {{ $value }}  metric_points'
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#queue-length'
    - alert: SendQueueFailedlog_records
      expr: rate(otelcol_exporter_enqueue_failed_log_records_total[5m]) > 0
      for: 1m
      labels:
        severity: warning
      annotations:
        description: 'The {{ $labels.exporter }} sending queue failed to accept {{ $value }}  log_records'
        runbook_url: 'https://opentelemetry.io/docs/collector/internal-telemetry/#queue-length'
